{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TT8xdQaA0tek"
   },
   "source": [
    "# MapChat: Content Generation\n",
    "\n",
    "[MapChat GitHub](https://github.com/csci-599-applied-ml-for-games/MapChat)\n",
    "\n",
    "[textgenrnn Github](https://github.com/minimaxir/textgenrnn)\n",
    "\n",
    "[Yelp Dataset](https://www.yelp.com/dataset) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuJWj7Sa1bN2"
   },
   "source": [
    "## Clone the MapChat repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9PW2NdHhyq06",
    "outputId": "17ec95a1-c47b-4bd8-f7b9-4d61d185a871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Thotiana Oblock/Desktop/USC/SPRING2020/CSCI599/MapChat/content/textgenrnn\r\n"
     ]
    }
   ],
   "source": [
    "# ! git clone https://github.com/csci-599-applied-ml-for-games/MapChat.git\n",
    "\n",
    "# ! pip list\n",
    "# ! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pH7nqog71s4E"
   },
   "source": [
    "## Install and Import required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgbImW5M058L"
   },
   "outputs": [],
   "source": [
    "!pip install -q textgenrnn\n",
    "# from google.colab import files\n",
    "from textgenrnn import textgenrnn\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BxvFBSpY2BR3"
   },
   "source": [
    "## Textgenrnn model configuration\n",
    "Check out [demo](https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb) for more info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9al9Qsb1Kjj"
   },
   "outputs": [],
   "source": [
    "model_cfg = {\n",
    "    'word_level': False,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n",
    "    'rnn_size': 128,   # number of LSTM cells of each layer (128/256 recommended)\n",
    "    'rnn_layers': 3,   # number of LSTM layers (>=2 recommended)\n",
    "    'rnn_bidirectional': False,   # consider text both forwards and backward, can give a training boost\n",
    "    'max_length': 30,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n",
    "    'max_words': 10000,   # maximum number of words to model; the rest will be ignored (word-level model only)\n",
    "}\n",
    "\n",
    "train_cfg = {\n",
    "    'line_delimited': True,   # set to True if each text has its own line in the source file\n",
    "    'num_epochs': 20,   # set higher to train the model for longer\n",
    "    'gen_epochs': 10,   # generates sample text from model after given number of epochs\n",
    "    'train_size': 0.8,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n",
    "    'dropout': 0.0,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n",
    "    'validation': False,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n",
    "    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7xCuRXR2kkr"
   },
   "source": [
    "## Connect text file to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wsYPopW2w36"
   },
   "outputs": [],
   "source": [
    "model_name = \"grocery\"   # change to set file name of resulting trained models/text\n",
    "file_name = \"%s.txt\" % (model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ9_s5AN_EC7"
   },
   "source": [
    "## Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-kEeMLnc_PVP",
    "outputId": "08ebd070-a270-4124-ae0e-d661f7ea1644"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40,185 texts collected.\n",
      "Training new model w/ 3-layer, 128-cell LSTMs\n",
      "Training on 2,245,307 character sequences.\n",
      "Epoch 1/20\n",
      "2192/2192 [==============================] - 159s 73ms/step - loss: 1.6366\n",
      "Epoch 2/20\n",
      "2192/2192 [==============================] - 157s 71ms/step - loss: 1.1379\n",
      "Epoch 3/20\n",
      "2192/2192 [==============================] - 158s 72ms/step - loss: 1.0710\n",
      "Epoch 4/20\n",
      "2192/2192 [==============================] - 160s 73ms/step - loss: 1.0344\n",
      "Epoch 5/20\n",
      "2192/2192 [==============================] - 159s 72ms/step - loss: 1.0080\n",
      "Epoch 6/20\n",
      "2192/2192 [==============================] - 158s 72ms/step - loss: 0.9881\n",
      "Epoch 7/20\n",
      "2192/2192 [==============================] - 157s 72ms/step - loss: 0.9716\n",
      "Epoch 8/20\n",
      "2192/2192 [==============================] - 157s 71ms/step - loss: 0.9573\n",
      "Epoch 9/20\n",
      "2192/2192 [==============================] - 156s 71ms/step - loss: 0.9440\n",
      "Epoch 10/20\n",
      "2192/2192 [==============================] - 155s 71ms/step - loss: 0.9312\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "The food is always fresh and the service was fresh and delicious.\n",
      "\n",
      "The food is always fresh and the service was delicious and the service is good.\n",
      "\n",
      "They have a great selection of food and the staff was amazing.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "The food is good and the staff is great.\n",
      "\n",
      "The food is great and the service is good.\n",
      "\n",
      "The food is so good and the service was good and the food was delicious and the service is very good.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "The sandwich was very friendly and the staff was very friendly.\n",
      "\n",
      "Great to find this food city we true market is serving a few funds to the atmosphere!\n",
      "\n",
      "In the morning was worth browsing and easy to order foodstuffs/such as, chinese, healthy foods and to its healthy!\n",
      "\n",
      "Epoch 11/20\n",
      "2192/2192 [==============================] - 155s 71ms/step - loss: 0.9194\n",
      "Epoch 12/20\n",
      "2192/2192 [==============================] - 155s 71ms/step - loss: 0.9076\n",
      "Epoch 13/20\n",
      "2192/2192 [==============================] - 155s 71ms/step - loss: 0.8960\n",
      "Epoch 14/20\n",
      "2192/2192 [==============================] - 155s 71ms/step - loss: 0.8845\n",
      "Epoch 15/20\n",
      "2192/2192 [==============================] - 155s 71ms/step - loss: 0.8729\n",
      "Epoch 16/20\n",
      "2192/2192 [==============================] - 154s 70ms/step - loss: 0.8612\n",
      "Epoch 17/20\n",
      "2192/2192 [==============================] - 157s 72ms/step - loss: 0.8492\n",
      "Epoch 18/20\n",
      "2192/2192 [==============================] - 154s 70ms/step - loss: 0.8375\n",
      "Epoch 19/20\n",
      "2192/2192 [==============================] - 154s 70ms/step - loss: 0.8257\n",
      "Epoch 20/20\n",
      "2192/2192 [==============================] - 155s 71ms/step - loss: 0.8144\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "The food is always fresh and the service is great and the food is great.\n",
      "\n",
      "The food is always good and the service is great.\n",
      "\n",
      "The food is always fresh and the service is great.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "She has great prices on cooked food, they also have a great selection of fresh food and the prices are reasonable.\n",
      "\n",
      "He was always surprised the food is always fresh and the prices are reasonable.\n",
      "\n",
      "The food is amazing and the service was great.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Chocolate chips and using your food occasioning how fast, just organic products they will deliver good food available in my butcher.\n",
      "\n",
      "Prices are cheap and delicious.\n",
      "\n",
      "As for the winns to anyone who served me of a while!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen = textgenrnn(name=model_name)\n",
    "\n",
    "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
    "\n",
    "train_function(\n",
    "    file_path=file_name,\n",
    "    new_model=True,\n",
    "    num_epochs=train_cfg['num_epochs'],\n",
    "    gen_epochs=train_cfg['gen_epochs'],\n",
    "    batch_size=1024,\n",
    "    train_size=train_cfg['train_size'],\n",
    "    dropout=train_cfg['dropout'],\n",
    "    validation=train_cfg['validation'],\n",
    "    is_csv=train_cfg['is_csv'],\n",
    "    rnn_layers=model_cfg['rnn_layers'],\n",
    "    rnn_size=model_cfg['rnn_size'],\n",
    "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
    "    max_length=model_cfg['max_length'],\n",
    "    dim_embeddings=100,\n",
    "    word_level=model_cfg['word_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htZTUyUuCln1"
   },
   "source": [
    "## Generate and save content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "MlfiJ7x0CqCp",
    "outputId": "b86c3a11-3cc2-45e5-bef5-e1c14871f3ce"
   },
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-96d38f4715c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                          \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                          max_gen_length=max_gen_length)\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
     ]
    }
   ],
   "source": [
    "temperature = [1.0, 0.5, 0.2, 0.2]   \n",
    "prefix = None   # if you want each generated text to start with a given seed text\n",
    "\n",
    "if train_cfg['line_delimited']:\n",
    "  n = 1000\n",
    "  max_gen_length = 60 if model_cfg['word_level'] else 300\n",
    "else:\n",
    "  n = 1\n",
    "  max_gen_length = 2000 if model_cfg['word_level'] else 10000\n",
    "  \n",
    "timestring = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "gen_file = '{}_gentext_{}.txt'.format(model_name, timestring)\n",
    "\n",
    "textgen.generate_to_file(gen_file,\n",
    "                         temperature=temperature,\n",
    "                         prefix=prefix, \n",
    "                         n=n,\n",
    "                         max_gen_length=max_gen_length)\n",
    "# files.download(gen_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmxaHtP5C8fl"
   },
   "source": [
    "## Save weights and config files to recreate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zvm_V4f3DKst"
   },
   "outputs": [],
   "source": [
    "# files.download('{}_weights.hdf5'.format(model_name))\n",
    "# files.download('{}_vocab.json'.format(model_name))\n",
    "# files.download('{}_config.json'.format(model_name))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "contentgen.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
